{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/Speaker_Verification_version_1.0/Speaker-Verification\n"
     ]
    }
   ],
   "source": [
    "cd /workdir/Speaker_Verification_version_1.0/Speaker-Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification import transforms as T\n",
    "from speaker_verification.dataset import SpeakingFacesDataset\n",
    "from speaker_verification.dataset import ValidDataset\n",
    "from speaker_verification.sampler import ProtoSampler\n",
    "from speaker_verification.sampler import ValidSampler\n",
    "from speaker_verification.models_handmade.resnet import ResNet34\n",
    "from speaker_verification.models import ResNet\n",
    "from speaker_verification.loss import PrototypicalLoss\n",
    "from speaker_verification.train import train_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from speaker_verification.utils import plot_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet1 (Resnet from Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = ResNet(pretrained_weights=True, \n",
    "                        fine_tune=True, \n",
    "                        embedding_size=128, \n",
    "                        modality = \"rgb\", \n",
    "                        filter_size=\"default\", \n",
    "                        from_torch=True\n",
    "                    )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 4.09436205625534\n",
      "Average train accuracy: 1.5500000739097595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval (epoch = 0): 100%|██████████| 594/594 [01:17<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 48.8092462548469\n",
      "\n",
      "Average val accuracy: 50.56642817059483\n",
      "Best eer model saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [01:48<00:00, 108.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc model saved at epoch 0\n",
      "Time elapsed: 1.800370344084998  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"annotations_file_short_SF.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=image_transform, \n",
    "                                    audio_transform=T.audio_transform)\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                        image_transform=image_transform, \n",
    "                        audio_transform=T.audio_transform)\n",
    "\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(train_dataset.labels,\n",
    "                            n_batch=200,\n",
    "                            n_ways=60, # n_way\n",
    "                            n_support=1, # n_shots\n",
    "                            n_query=1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                        batch_sampler=train_sampler,\n",
    "                        num_workers=4, pin_memory=True\n",
    "                        )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4, \n",
    "                        pin_memory=True)\n",
    "\n",
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='cosine_similarity')\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/Speaker_Verification_version_1.0/results\",\n",
    "                    exp_name=\"chern\",\n",
    "                    modality=\"rgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth\" to /workdir/data/.torch/hub/checkpoints/resnet34-43635321.pth\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet34', pretrained=True, num_classes=128, in_chans=3)\n",
    "image_transform = T.image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name = [name for name,_ in model.named_parameters()] # All parameters name\n",
    "layer_name = [name for name,_ in model.named_modules()] # All layers name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 4.09741884469986\n",
      "Average train accuracy: 1.8833334314823151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval (epoch = 0): 100%|██████████| 594/594 [01:17<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 49.467722624598146\n",
      "\n",
      "Average val accuracy: 50.57519640852974\n",
      "Best eer model saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [01:48<00:00, 108.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc model saved at epoch 0\n",
      "Time elapsed: 1.8148329654708504  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"annotations_file_short_SF.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=image_transform, \n",
    "                                    audio_transform=T.audio_transform)\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                        image_transform=image_transform, \n",
    "                        audio_transform=T.audio_transform)\n",
    "\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(train_dataset.labels,\n",
    "                            n_batch=200,\n",
    "                            n_ways=60, # n_way\n",
    "                            n_support=1, # n_shots\n",
    "                            n_query=1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                        batch_sampler=train_sampler,\n",
    "                        num_workers=4, pin_memory=True\n",
    "                        )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4, \n",
    "                        pin_memory=True)\n",
    "\n",
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='cosine_similarity')\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/Speaker_Verification_version_1.0/results\",\n",
    "                    exp_name=\"chern\",\n",
    "                    modality=\"rgb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
