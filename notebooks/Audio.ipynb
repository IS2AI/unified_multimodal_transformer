{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/Speaker_Verification_version_1.0/Speaker-Verification\n"
     ]
    }
   ],
   "source": [
    "cd /workdir/Speaker_Verification_version_1.0/Speaker-Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification import transforms as T\n",
    "from speaker_verification.dataset import SpeakingFacesDataset\n",
    "from speaker_verification.dataset import ValidDataset\n",
    "from speaker_verification.sampler import ProtoSampler\n",
    "from speaker_verification.sampler import ValidSampler\n",
    "from speaker_verification.models_handmade.resnet import ResNet34\n",
    "from speaker_verification.models import SelfAttentivePool2d\n",
    "from speaker_verification.loss import PrototypicalLoss\n",
    "from speaker_verification.train import train_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from speaker_verification.utils import plot_sample\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from speaker_verification.loss import PrototypicalLoss\n",
    "from speaker_verification.metrics import EER_\n",
    "from speaker_verification.metrics import accuracy_\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/annotations_file_short.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=T.image_transform, \n",
    "                                    audio_transform=T.audio_transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=T.image_transform, \n",
    "                            audio_transform=T.audio_transform)\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model = ResNet34()\n",
    "model = timm.create_model('resnet34', pretrained=True, num_classes=128, in_chans=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 12.77603850364685\n",
      "Average train accuracy: 43.333336639404294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval (epoch = 0): 100%|██████████| 594/594 [01:16<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 40.396064017445006\n",
      "\n",
      "Average val accuracy: 59.49687850729518\n",
      "Best eer model saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [01:19<00:00, 79.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc model saved at epoch 0\n",
      "Time elapsed: 1.327319642699634  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/results\",\n",
    "                    exp_name=\"resnet\",\n",
    "                    modality='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timm REsNet + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/annotations_file_short.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=T.image_transform, \n",
    "                                    audio_transform=T.audio_transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=T.image_transform, \n",
    "                            audio_transform=T.audio_transform)\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model = ResNet34()\n",
    "model = timm.create_model('resnet34', pretrained=True, num_classes=128, in_chans=1)\n",
    "model.global_pool = SelfAttentivePool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 16.273793411254882\n",
      "Average train accuracy: 40.000003051757815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval (epoch = 0): 100%|██████████| 594/594 [01:16<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 39.018451303037835\n",
      "\n",
      "Average val accuracy: 60.91820987654321\n",
      "Best eer model saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [01:19<00:00, 79.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc model saved at epoch 0\n",
      "Time elapsed: 1.3235322367555151  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "\n",
    "                    \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/results\",\n",
    "                    exp_name=\"resnet_sap\",\n",
    "                    modality='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try AST (Audio Spectrogram Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2wav1 = \"/workdir/sf_pv/data_v2/sub_1/11/wav/574.wav\"\n",
    "path2wav2 = \"/workdir/sf_pv/data_v2/sub_1/11/wav/603.wav\"\n",
    "path2wav3 = \"/workdir/sf_pv/data_v2/sub_1/12/wav/673.wav\"\n",
    "data_wav_id1_1, sample_rate = soundfile.read(path2wav1) \n",
    "data_wav_id1_2, sample_rate = soundfile.read(path2wav2) \n",
    "data_wav_id2_1, sample_rate = soundfile.read(path2wav3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute attention masks and normalize the waveform if needed\n",
    "inputs1 = feature_extractor(data_wav_id1_1, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "inputs2 = feature_extractor(data_wav_id1_2, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "inputs3 = feature_extractor(data_wav_id2_1, sampling_rate=16000, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[  1.9893,  -0.9755,  -3.4737,  -8.1023,  -3.6725,  -1.6364, -10.6159,\n",
       "          -5.0053,  -9.0250,  -7.0192,  -6.4388,  -7.6345, -11.6691,  -9.4656,\n",
       "          -8.6656,  -9.7345,  -7.3644, -10.0390,  -7.8844,  -8.0277,  -9.8606,\n",
       "          -6.1820,  -9.0928,  -8.2744,  -6.3399,  -7.7557,  -6.7260,  -8.4375,\n",
       "          -9.7831, -10.2545, -10.2305,  -8.4399,  -8.6328,  -9.8559, -10.6823,\n",
       "         -11.9752, -10.3021, -10.4324,  -7.3540,  -7.7377,  -7.7613,  -6.9020,\n",
       "          -9.7197,  -9.2440,  -4.5852,  -9.9752,  -4.9876,  -9.7073,  -5.3093,\n",
       "          -6.9735, -10.4641,  -7.8191,  -7.2847,  -8.4210,  -8.9820,  -6.6953,\n",
       "          -8.4857,  -8.5635,  -8.1789,  -8.9217,  -8.0305,  -9.7148,  -7.9259,\n",
       "          -7.2844,  -9.4872,  -9.9177,  -9.0284,  -8.0984, -10.8437,  -8.1701,\n",
       "          -8.6471, -12.7541,  -4.9552,  -6.0202,  -7.7936, -10.2052,  -8.7268,\n",
       "          -9.1717,  -8.5424,  -8.8025,  -8.3987,  -6.3925,  -9.0607,  -6.5005,\n",
       "          -7.3885,  -7.6809,  -7.6156,  -8.2383,  -7.8657,  -8.2470,  -7.9948,\n",
       "          -8.1724, -10.2166, -10.7534,  -8.2227,  -9.8718,  -7.2492,  -7.4438,\n",
       "          -7.9490,  -8.0937,  -8.8765,  -8.9808,  -9.6581,  -9.5389,  -8.7116,\n",
       "          -7.4869, -10.2372,  -9.2495, -10.0977,  -9.4247,  -8.2787,  -7.0681,\n",
       "          -8.1907,  -8.4543,  -8.7218,  -8.8933,  -8.5608,  -7.8743,  -8.2204,\n",
       "          -6.9156,  -7.7004, -10.3125,  -8.8513, -11.0455,  -7.8518, -10.9941,\n",
       "          -8.5412,  -9.8674, -10.0910,  -9.9037,  -6.0924,  -9.7876,  -7.2879,\n",
       "          -7.9779,  -7.6340,  -8.0530, -12.4868,  -4.9323,  -6.3449,  -7.5242,\n",
       "          -8.0898,  -9.1375,  -9.4042,  -9.2734, -10.7561, -10.2216,  -8.6891,\n",
       "          -9.7934, -10.2026, -10.8886, -10.6824, -10.1178,  -8.5819,  -9.1724,\n",
       "          -9.9833, -10.6677,  -9.7881, -10.3781, -10.5889,  -9.3050,  -9.9706,\n",
       "          -7.9122,  -8.4482, -11.2228,  -8.0396,  -9.5130,  -9.4796, -10.1535,\n",
       "          -9.4611,  -9.4621, -10.7580,  -9.7650, -10.2539,  -9.4774,  -9.5719,\n",
       "         -10.7713, -10.3360,  -8.7853,  -8.7260,  -9.0233,  -7.9260,  -9.2219,\n",
       "          -9.9253, -12.2336, -10.1912,  -9.3876,  -9.4795,  -9.9934, -11.0861,\n",
       "          -8.7584, -11.1891,  -8.2794,  -8.3817,  -9.9686,  -9.5064,  -9.6737,\n",
       "         -10.2100, -10.8631, -10.9921, -10.3709,  -7.7796,  -9.3775,  -8.6406,\n",
       "          -8.8155,  -7.7965,  -8.0306,  -9.0648,  -9.6236, -11.9727, -10.7349,\n",
       "         -11.2058, -10.6412, -11.5054, -10.1332,  -8.9866, -12.2552,  -9.1344,\n",
       "         -10.5456, -11.3877,  -9.8179,  -9.2567, -10.3011, -10.3780, -10.0239,\n",
       "          -8.8762, -10.8294,  -9.8575, -10.3108, -11.3770,  -9.4063, -11.5815,\n",
       "          -9.1793, -10.1044, -10.1409, -10.4892,  -9.2096, -10.3688,  -9.1863,\n",
       "         -10.1843,  -8.9008, -10.9272,  -9.6215,  -9.6748, -10.2558,  -9.3086,\n",
       "         -10.8379, -10.1906, -10.8592, -10.4342, -11.2741, -10.3527, -10.0480,\n",
       "         -11.5294,  -9.9135, -10.9815, -10.8663, -11.2761, -11.6443,  -9.8875,\n",
       "         -10.2952, -10.2373, -11.2451, -11.7824, -11.5869, -10.3383,  -9.1204,\n",
       "         -10.0475,  -8.0939,  -8.7504,  -8.5889,  -8.1486,  -9.8723,  -8.5317,\n",
       "         -10.1626,  -9.9090, -10.6233, -10.6307,  -9.4334, -10.1299,  -9.0169,\n",
       "          -9.0370, -10.0984,  -8.9345,  -8.8324,  -7.8694,  -8.4024, -10.7076,\n",
       "          -9.5834,  -7.4587,  -8.6416,  -8.7609,  -8.0692,  -9.7665,  -9.4746,\n",
       "          -9.4642,  -9.2640,  -9.5144,  -7.9506,  -9.5147,  -7.7411,  -5.5963,\n",
       "          -7.9952,  -8.7408,  -9.8612,  -8.6331,  -9.9919,  -7.9874,  -6.8087,\n",
       "          -7.9032, -10.0722, -10.2224,  -9.0601,  -9.9291,  -9.4628,  -7.4155,\n",
       "          -9.8817,  -8.4450,  -8.3106, -10.3994,  -9.0561,  -8.7889,  -7.5122,\n",
       "          -9.2737,  -9.4860,  -9.9106, -10.6559,  -9.1438, -10.9878,  -9.6310,\n",
       "          -8.8416, -10.3501, -10.6117,  -9.2055, -11.9310,  -9.9483,  -9.4731,\n",
       "         -10.8069,  -9.1163, -10.4494,  -9.5496, -10.0400,  -8.1116, -11.3726,\n",
       "          -7.2487, -10.5004, -11.2577, -10.4859, -10.4531,  -8.0224,  -8.3271,\n",
       "         -10.8516,  -8.6735,  -7.8412,  -7.6472,  -5.6915,  -7.5159,  -7.7677,\n",
       "          -5.5404,  -6.0942,  -8.3853,  -6.3836,  -7.9748,  -9.5098,  -7.6267,\n",
       "          -7.3742,  -8.2682,  -7.9879,  -7.9704,  -6.0880,  -8.4777,  -7.8102,\n",
       "          -8.1720,  -9.4036,  -8.8371,  -9.9580,  -9.2749,  -8.7466,  -9.6429,\n",
       "          -6.2611,  -8.7779,  -8.5628,  -8.1661,  -7.8476,  -7.4914,  -8.8373,\n",
       "          -9.5690,  -8.4259,  -7.9935,  -9.1482,  -6.5480,  -7.3467,  -7.7697,\n",
       "          -8.8740,  -8.6659,  -8.1893,  -9.0294,  -9.0359, -10.2876,  -7.1495,\n",
       "          -9.3897,  -9.3911,  -9.3135,  -8.7107,  -9.5479,  -7.5689,  -9.6340,\n",
       "         -10.0659,  -6.8964,  -7.0054,  -9.3618,  -9.7094,  -9.2366,  -9.4245,\n",
       "          -7.6196,  -7.5400,  -7.8753,  -8.1143,  -5.2762,  -7.3354,  -9.7611,\n",
       "         -11.8089,  -9.2209, -10.2287, -10.5433,  -7.9615,  -8.6862,  -7.7149,\n",
       "          -8.8058,  -9.7858, -10.4538,  -9.1859, -10.1769, -10.7744, -10.2058,\n",
       "          -7.3301,  -9.0991, -12.0244,  -8.2670,  -8.0715, -12.6677,  -9.4126,\n",
       "          -8.0552,  -5.6735, -10.5825,  -7.5519, -12.2845,  -8.8466,  -7.8252,\n",
       "          -6.9481, -10.5882,  -8.3866,  -6.6743,  -8.3146,  -6.1352,  -8.1502,\n",
       "          -9.4071,  -7.8427,  -8.0650,  -8.6734,  -6.5060,  -8.4402,  -9.7071,\n",
       "         -10.6898, -10.3877, -11.0085, -10.4318,  -7.3520,  -5.5162,  -8.6045,\n",
       "          -8.6366,  -7.7427, -10.5851,  -8.2372,  -6.9988,  -9.3808,  -7.0024,\n",
       "          -8.2311,  -9.4276, -12.9631, -10.6417,  -8.9993,  -6.0887,  -6.3413,\n",
       "          -7.0284,  -8.4063, -10.3238,  -9.2370,  -6.6040,  -7.4847,  -6.5540,\n",
       "          -7.5272,  -8.4731, -11.1709,  -8.5873,  -8.9127,  -9.1224,  -7.4515,\n",
       "         -12.2368,  -9.7206,  -8.4676,  -5.7617,  -8.5165,  -8.5659,  -9.1371,\n",
       "          -5.1056, -10.6390,  -4.4223,  -6.0213,  -8.0206,  -7.2482,  -7.5069,\n",
       "          -7.8440,  -7.8023, -12.8602,  -8.7489,  -6.4862,  -9.0419, -10.1670,\n",
       "         -11.3821,  -8.3622,  -6.2875,  -9.6326,  -9.6260,  -7.7578,  -4.8811,\n",
       "          -7.6827,  -8.1512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs3.input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model(**inputs1).logits\n",
    "output2 = model(**inputs2).logits\n",
    "output3 = model(**inputs3).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9987], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings1 = torch.nn.functional.normalize(output1, dim=-1).cpu()\n",
    "embeddings2 = torch.nn.functional.normalize(output2, dim=-1).cpu()\n",
    "emb3 = torch.nn.functional.normalize(output3, dim=-1).cpu()\n",
    "\n",
    "# the resulting embeddings can be used for cosine similarity-based retrieval\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarity = cosine_sim(embeddings1, emb3)\n",
    "print(similarity)\n",
    "threshold = 0.86  # the optimal threshold is dataset-dependent\n",
    "if similarity < threshold:\n",
    "    print(\"Speakers are not the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"max_length\": 1024,\n",
       "  \"mean\": -4.2677393,\n",
       "  \"num_mel_bins\": 128,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"std\": 4.5689974\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ast_transform(data_wav, sample_rate):\n",
    "    data_wav = data_wav.squeeze().numpy()\n",
    "    inputs = feature_extractor(data_wav, sampling_rate=sample_rate, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/annotations_file_short.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=T.image_transform, \n",
    "                                    audio_transform=ast_transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=T.image_transform, \n",
    "                            audio_transform=ast_transform)\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [6, 1, 1024, 1, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workdir/results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/speaker_verification/train.py:43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, valid_dataloader, train_sampler, criterion, optimizer, scheduler, device, num_epochs, save_dir, exp_name, modality)\u001b[0m\n\u001b[1;32m     40\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     41\u001b[0m start_train \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> 43\u001b[0m model, train_loss, train_acc \u001b[39m=\u001b[39m train_singe_epoch(model, \n\u001b[1;32m     44\u001b[0m                           train_dataloader,\n\u001b[1;32m     45\u001b[0m                           epoch, \n\u001b[1;32m     46\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_ways, \n\u001b[1;32m     47\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_shots, \n\u001b[1;32m     48\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_query,\n\u001b[1;32m     49\u001b[0m                           criterion,\n\u001b[1;32m     50\u001b[0m                           optimizer,\n\u001b[1;32m     51\u001b[0m                           device,\n\u001b[1;32m     52\u001b[0m                           modality)\n\u001b[1;32m     54\u001b[0m end_train \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     55\u001b[0m logs[\u001b[39m'\u001b[39m\u001b[39mtrain_time_min\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend((end_train \u001b[39m-\u001b[39m start_train)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m)\n",
      "File \u001b[0;32m/workdir/speaker_verification/train.py:127\u001b[0m, in \u001b[0;36mtrain_singe_epoch\u001b[0;34m(model, train_dataloader, epoch, n_ways, n_shots, n_query, criterion, optimizer, device, modality)\u001b[0m\n\u001b[1;32m    124\u001b[0m     wav, _, _, _ \u001b[39m=\u001b[39m batch \u001b[39m# we do not use labels from dataset\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     data \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 127\u001b[0m data \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m    129\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(n_ways)\u001b[39m.\u001b[39mrepeat(n_query)\n\u001b[1;32m    130\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:583\u001b[0m, in \u001b[0;36mASTForAudioClassification.forward\u001b[0;34m(self, input_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m    Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    581\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 583\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_spectrogram_transformer(\n\u001b[1;32m    584\u001b[0m     input_values,\n\u001b[1;32m    585\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    586\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    587\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    588\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    591\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    592\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:500\u001b[0m, in \u001b[0;36mASTModel.forward\u001b[0;34m(self, input_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    498\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 500\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_values)\n\u001b[1;32m    502\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    503\u001b[0m     embedding_output,\n\u001b[1;32m    504\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    509\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:82\u001b[0m, in \u001b[0;36mASTEmbeddings.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_values: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     81\u001b[0m     batch_size \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(input_values)\n\u001b[1;32m     84\u001b[0m     cls_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m     distillation_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistillation_token\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:113\u001b[0m, in \u001b[0;36mASTPatchEmbeddings.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    111\u001b[0m input_values \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    112\u001b[0m input_values \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection(input_values)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [6, 1, 1024, 1, 128]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/results\",\n",
    "                    exp_name=\"AST\",\n",
    "                    modality='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Evaluation as from Korean guys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/annotations_file_short.csv\"\n",
    "DATASET_DIR = '/workdir/sf_pv/data_v2'\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=None, \n",
    "                            audio_transform=None)\n",
    "\n",
    "# valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "#                             batch_size=1,\n",
    "#                             shuffle=True,\n",
    "#                             num_workers=4, \n",
    "#                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/sf_pv/data_v2/sub_101/23/wav/139.wav\n",
      "/workdir/sf_pv/data_v2/sub_101/25/wav/49.wav\n"
     ]
    }
   ],
   "source": [
    "sample = valid_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load audio from torchaudio and from soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2wav1 = \"/workdir/sf_pv/data_v2/sub_101/23/wav/139.wav\"\n",
    "path2wav2 = \"/workdir/sf_pv/data_v2/sub_101/25/wav/49.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wav, sample_rate = torchaudio.load(path2wav1) \n",
    "audio1, sample_rate1 = soundfile.read(path2wav1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(sample_rate)\n",
    "print(sample_rate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0045, 0.0071, 0.0056,  ..., 0.0119, 0.0111, 0.0084]])\n"
     ]
    }
   ],
   "source": [
    "print(data_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00445557 0.00714111 0.00561523 ... 0.01193237 0.01113892 0.00842285]\n"
     ]
    }
   ],
   "source": [
    "print(audio1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_dataloader:\n",
    "    id1, id2, labels = batch\n",
    "\n",
    "    wav_id1, rgb_id1, thr_id1, person_id1 = id1\n",
    "    wav_id2, rgb_id2, thr_id2, person_id2 = id2\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same person: True\n",
      "tensor([1])\n",
      "First person id: tensor([113])\n",
      "Second person id: tensor([113])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Same person: {bool(labels)}\")\n",
    "print(labels)\n",
    "\n",
    "print(f\"First person id: {person_id1}\")\n",
    "print(f\"Second person id: {person_id2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same person: True\n",
      "First person id: 101\n",
      "Second person id: 101\n"
     ]
    }
   ],
   "source": [
    "for sample in valid_dataset:\n",
    "    id1, id2, label = sample\n",
    "\n",
    "    print(f\"Same person: {bool(label)}\")\n",
    "\n",
    "    wav_id1, rgb_id1, thr_id1, person_id1 = id1\n",
    "    wav_id2, rgb_id2, thr_id2, person_id2 = id2\n",
    "\n",
    "    print(f\"First person id: {person_id1}\")\n",
    "    print(f\"Second person id: {person_id2}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpeechBrain embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/speechbrain/lobes/models/Xvector.py:102\u001b[0m, in \u001b[0;36mXvector.forward\u001b[0;34m(self, x, lens)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m layer(x, lengths\u001b[39m=\u001b[39;49mlens)\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'lengths'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_id1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/speechbrain/pretrained/interfaces.py:945\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    943\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmods\u001b[39m.\u001b[39mcompute_features(wavs)\n\u001b[1;32m    944\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmods\u001b[39m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[0;32m--> 945\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmods\u001b[39m.\u001b[39;49membedding_model(feats, wav_lens)\n\u001b[1;32m    946\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n\u001b[1;32m    947\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mmean_var_norm_emb(\n\u001b[1;32m    948\u001b[0m         embeddings, torch\u001b[39m.\u001b[39mones(embeddings\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    949\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/speechbrain/lobes/models/Xvector.py:104\u001b[0m, in \u001b[0;36mXvector.forward\u001b[0;34m(self, x, lens)\u001b[0m\n\u001b[1;32m    102\u001b[0m         x \u001b[39m=\u001b[39m layer(x, lengths\u001b[39m=\u001b[39mlens)\n\u001b[1;32m    103\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/speechbrain/nnet/CNN.py:422\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    419\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 422\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_manage_padding(\n\u001b[1;32m    423\u001b[0m         x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcausal\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    427\u001b[0m     num_pad \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/speechbrain/nnet/CNN.py:474\u001b[0m, in \u001b[0;36mConv1d._manage_padding\u001b[0;34m(self, x, kernel_size, dilation, stride)\u001b[0m\n\u001b[1;32m    471\u001b[0m padding \u001b[39m=\u001b[39m get_padding_elem(L_in, stride, kernel_size, dilation)\n\u001b[1;32m    473\u001b[0m \u001b[39m# Applying padding\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mpad(x, padding, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode)\n\u001b[1;32m    476\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now"
     ]
    }
   ],
   "source": [
    "classifier.encode_batch(wav_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_id1 = classifier.encode_batch(wav_id1[0])\n",
    "embeddings_id2 = classifier.encode_batch(wav_id2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = F.cosine_similarity(embeddings_id1, embeddings_id2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEER_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcos_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/Speaker_Verification_version_1.0/Speaker-Verification/speaker_verification/metrics.py:9\u001b[0m, in \u001b[0;36mEER_\u001b[0;34m(cos_sim, labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mEER_\u001b[39m(cos_sim, labels):\n\u001b[1;32m      8\u001b[0m     cos_sim \u001b[39m=\u001b[39m cos_sim\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m----> 9\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m     fpr, tpr, threshold \u001b[39m=\u001b[39m roc_curve(labels, cos_sim, pos_label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     fnr \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m tpr\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "EER_(cos_sim, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer, scores = EER_(cos_sim, label)\n",
    "accuracy = accuracy_(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "score, prediction = verification.verify_files(file1, file2)\n",
    "\n",
    "print(score)\n",
    "print(prediction) # True = same speaker, False=Different speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавила варьирование параметров для transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "library=\"timm\"\n",
    "modality = \"wav\"\n",
    "model_name = \"resnet34\"\n",
    "pool=\"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=True, \n",
    "                fine_tune=False, \n",
    "                embedding_size=128, \n",
    "                modality = modality,\n",
    "                model_name = model_name,\n",
    "                pool=pool)\n",
    "model = model.to(device)\n",
    "\n",
    "audio_T = Audio_Transforms(sample_rate=16000,\n",
    "                            sample_duration=3, # seconds\n",
    "                            n_fft=512, # from Korean code\n",
    "                            win_length=400,\n",
    "                            hop_length=160,\n",
    "                            window_fn=torch.hamming_window,\n",
    "                            n_mels=40)\n",
    "\n",
    "image_T = Image_Transforms(model,\n",
    "                            library=library,\n",
    "                            model_name = model_name,\n",
    "                            resize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/Speaker_Verification_version_1.0/Speaker-Verification/annotations_file_short_SF.csv\"\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "DATASET_DIR = f'{PATH2DATASET}/data_v2'\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=image_T.transform, \n",
    "                                    audio_transform=audio_T.transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=image_T.transform, \n",
    "                            audio_transform=audio_T.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeeda1b463844374b4d7e8b233ca8bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613ea580c6a44b7ea9be2ee6266aa32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train (epoch = 0):   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 8.036772561073303\n",
      "Average train accuracy: 26.666668701171876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aeab9788794e92884f3a5d4a135fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (epoch = 0):   0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 38.135066470414856\n",
      "\n",
      "Average val accuracy: 61.7555765993266\n",
      "Best eer model saved at epoch 0\n",
      "Best acc model saved at epoch 0\n",
      "Time elapsed: 1.227491514896974  minutes\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/Speaker_Verification_version_1.0/results\",\n",
    "                    exp_name=\"chern\",\n",
    "                    modality=modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAP from Korean code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "library=\"timm\"\n",
    "modality = \"wav\"\n",
    "model_name = \"resnet34\"\n",
    "pool=\"SAP\"\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=True, \n",
    "                fine_tune=False, \n",
    "                embedding_size=128, \n",
    "                modality = modality,\n",
    "                model_name = model_name,\n",
    "                pool=pool)\n",
    "model = model.to(device)\n",
    "\n",
    "audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                            sample_duration=sample_duration, # seconds\n",
    "                            n_fft=n_fft, # from Korean code\n",
    "                            win_length=win_length,\n",
    "                            hop_length=hop_length,\n",
    "                            window_fn=window_fn,\n",
    "                            n_mels=n_mels)\n",
    "\n",
    "image_T = Image_Transforms(model,\n",
    "                            library=library,\n",
    "                            model_name = model_name,\n",
    "                            resize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/Speaker_Verification_version_1.0/Speaker-Verification/annotations_file_short_SF.csv\"\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "DATASET_DIR = f'{PATH2DATASET}/data_v2'\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=image_T.transform, \n",
    "                                    audio_transform=audio_T.transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=image_T.transform, \n",
    "                            audio_transform=audio_T.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, _, _, _ = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43084])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.shape # [n_channels, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(train_dataloader, desc=f'Train (epoch = {1})', leave=False)  \n",
    "\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "for batch in pbar:\n",
    "\n",
    "    if modality == \"rgb\":\n",
    "        # data_wav, data_rgb, data_thr, label\n",
    "        _,rgb, _, _ = batch # we do not use labels from dataset\n",
    "        data = rgb.to(device)\n",
    "    elif modality == \"thr\":\n",
    "        # data_wav, data_rgb, data_thr, label\n",
    "        _,_, thr, _ = batch # we do not use labels from dataset\n",
    "        data = thr.to(device)\n",
    "    elif modality == \"wav\":\n",
    "        wav, _, _, _ = batch # we do not use labels from dataset\n",
    "        data = wav.to(device)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 40, 201])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # [batch_size, n_channels, n_mels, number of frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implement step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = model(data)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try = torch.FloatTensor(6,512,40,201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "instancenorm = nn.InstanceNorm1d(n_mels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = \\tanh(Wx_t + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 40, 201])\n",
      "torch.Size([6, 1, 1, 201])\n",
      "torch.Size([6, 201, 1, 1])\n",
      "torch.Size([6, 201, 1])\n"
     ]
    }
   ],
   "source": [
    "x = instancenorm(data.squeeze()).unsqueeze(1).detach().cpu()\n",
    "print(x.shape) # [batch_size, n_channels, n_mels, number of frames]\n",
    "x = torch.mean(x, dim=2, keepdim=True)\n",
    "print(x.shape)\n",
    "x = x.permute(0,3,1,2) # [batch_size, number of frames,n_channels, n_mels]\n",
    "print(x.shape)\n",
    "x = x.squeeze(-1) # delete last dimension\n",
    "print(x.shape) # [batch_size, number of frames, n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nn.Linear(1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 201, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(W(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 201, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # [batch_size, number of frames, n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    out = nn.Parameter(torch.FloatTensor(*size))\n",
    "    nn.init.xavier_normal_(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = new_parameter(512,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 201, 1])\n",
      "torch.Size([6, 201])\n",
      "torch.Size([6, 201])\n",
      "torch.Size([6, 201, 1])\n"
     ]
    }
   ],
   "source": [
    "w = torch.matmul(h, u)\n",
    "print(w.shape)\n",
    "w = w.squeeze(dim=2)\n",
    "print(w.shape)\n",
    "w = F.softmax(w, dim=1)\n",
    "print(w.shape)\n",
    "w = w.view(x.size(0), x.size(1), 1)\n",
    "print(w.shape) # [batch_size, number of frames, n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "e = torch.sum(x * w, dim=1)\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.view(e.size()[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = e.view(e.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = nn.Sequential(*list(model.children())[:-2])\n",
    "model_new = model_new.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512, 2, 7])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = instancenorm(data.squeeze()).unsqueeze(1).detach()\n",
    "data_1 = model_new(data_1)\n",
    "data_1.shape # [batch_size, n_channels, n_mels, number of frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 512, 1, 7])\n",
      "torch.Size([6, 7, 512, 1])\n",
      "torch.Size([6, 7, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.mean(data_1, dim=2, keepdim=True) # [batch_size, n_channels, n_mels, number of frames]\n",
    "print(x.shape) \n",
    "x = x.permute(0,3,1,2) # [batch_size, number of frames,n_channels, n_mels]\n",
    "print(x.shape)\n",
    "x = x.squeeze(-1) # delete last dimension\n",
    "print(x.shape) # [batch_size, number of frames, n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 7, 512])\n"
     ]
    }
   ],
   "source": [
    "W = nn.Linear(512,512)\n",
    "W = W.to(device)\n",
    "h = torch.tanh(W(x)) # [batch_size, number of frames, n_channels]\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1])\n",
      "torch.Size([6, 7, 1])\n",
      "torch.Size([6, 7])\n",
      "torch.Size([6, 7])\n",
      "torch.Size([6, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "u = new_parameter(512,1).to(device)\n",
    "print(u.shape)\n",
    "w = torch.matmul(h, u)\n",
    "print(w.shape)\n",
    "w = w.squeeze(dim=2)\n",
    "print(w.shape)\n",
    "w = F.softmax(w, dim=1)\n",
    "print(w.shape)\n",
    "w = w.view(x.size(0), x.size(1), 1)\n",
    "print(w.shape) # [batch_size, number of frames, n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 512])\n"
     ]
    }
   ],
   "source": [
    "e = torch.sum(x * w, dim=1)\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.view(e.size()[0], -1).shape\n",
    "\n",
    "# x = self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(512,128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fc(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, n_channels, H, W]\n",
    "# [batch_size, 512, 1, 1]\n",
    "# [batch_size, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wite SAP as nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentivePool2d(nn.Module):\n",
    "    '''\n",
    "    Based on this article: https://www.isca-speech.org/archive/pdfs/odyssey_2018/cai18_odyssey.pdf\n",
    "    '''\n",
    "    def __init__(self, input_dim=512):\n",
    "        super(SelfAttentivePool2d, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.u = nn.Parameter(torch.FloatTensor(input_dim, 1))\n",
    "        nn.init.xavier_normal_(self.u)\n",
    "\n",
    "    def preprocess(self,x):\n",
    "        \"\"\"\n",
    "            x: [batch_size, n_channels, n_mels, number of frames] --> [batch_size, number of frames, n_channels]\n",
    "\n",
    "            step 1: [batch_size, n_channels, n_mels, number of frames] --> [batch_size, n_channels, 1, number of frames]\n",
    "            step 2: [batch_size, n_channels, 1, number of frames] --> [batch_size, number of frames, n_channels, 1]\n",
    "            step 3: [batch_size, number of frames, n_channels, 1] --> [batch_size, number of frames, n_channels]\n",
    "        \"\"\"\n",
    "        x = torch.mean(x, dim=2, keepdim=True) \n",
    "        x = x.permute(0,3,1,2)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        h = tanh(Wx + b)\n",
    "        w = Softmax(h @ u) * H\n",
    "        e = sum(w*x)\n",
    "        input:\n",
    "            x : [batch_size, n_channels, n_mels, number of frames]\n",
    "        \n",
    "        return:\n",
    "            e: size (batch_size, n_channels)\n",
    "        \"\"\"\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        h = torch.tanh(self.W(x))\n",
    "        w = torch.matmul(h, self.u).squeeze(dim=2) # [batch_size, number of frames, n_channels=1] --> squeeze: [batch_size, number of frames]\n",
    "        w = F.softmax(w, dim=1)\n",
    "        w = w.view(x.size(0), x.size(1), 1) # [batch_size, number of frames, n_channels=1]\n",
    "        e = torch.sum(x * w, dim=1) # utterance level representation e\n",
    "        e = e.view(e.size()[0], -1) # flatten\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.avgpool = SelfAttentivePool2d(model.fc.in_features)\n",
    "model.fc = nn.Linear(model.fc.in_features, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = instancenorm(data.squeeze()).unsqueeze(1).detach()\n",
    "x = model(x)\n",
    "x.shape # [batch_size, n_channels, n_mels, number of frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(model_name, pretrained=True, num_classes=128, in_chans=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.global_pool = SelfAttentivePool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = instancenorm(data.squeeze()).unsqueeze(1).detach()\n",
    "x = model(x)\n",
    "x.shape # [batch_size, n_channels, n_mels, number of frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add evaluation with divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "library=\"timm\"\n",
    "modality = \"wav\"\n",
    "model_name = \"resnet34\"\n",
    "pool=\"SAP\"\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=True, \n",
    "                fine_tune=False, \n",
    "                embedding_size=128, \n",
    "                modality = modality,\n",
    "                model_name = model_name,\n",
    "                pool=pool)\n",
    "model = model.to(device)\n",
    "\n",
    "audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                            sample_duration=sample_duration, # seconds\n",
    "                            n_fft=n_fft, # from Korean code\n",
    "                            win_length=win_length,\n",
    "                            hop_length=hop_length,\n",
    "                            window_fn=window_fn,\n",
    "                            n_mels=n_mels)\n",
    "\n",
    "image_T = Image_Transforms(model,\n",
    "                            library=library,\n",
    "                            model_name = model_name,\n",
    "                            resize=128)\n",
    "\n",
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/Speaker_Verification_version_1.0/Speaker-Verification/annotations_file_short_SF.csv\"\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "DATASET_DIR = f'{PATH2DATASET}/data_v2'\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=image_T.transform, \n",
    "                                    audio_transform=audio_T.transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=image_T.transform, \n",
    "                            audio_transform=None) #audio_T.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_transform(signal, sample_rate):\n",
    "    sample_duration = 16000\n",
    "    n_eval_cuts = 4\n",
    "    # Maximum audio length\n",
    "    max_audio = sample_duration * sample_rate\n",
    "    # stereo --> mono\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "    \n",
    "    audiosize = signal.shape[1] # time\n",
    "    if audiosize <= max_audio:\n",
    "        shortage = max_audio - audiosize\n",
    "        dim_padding = (0, shortage) # (left_pad, right_pad)\n",
    "        # ex: dim_padding = (1,2) --> [1,1,1] -> [0,1,1,1,0,0]\n",
    "        feat = torch.nn.functional.pad(signal, dim_padding, 'replicate') # shape: [n_channels, time]\n",
    "        feat = feat.unsqueeze(dim=0) # shape: [n_eval_cuts=1, n_channels, time]\n",
    "    else:\n",
    "        feats = []\n",
    "        startframe = torch.linspace(0,audiosize-max_audio,steps=n_eval_cuts)\n",
    "        for asf in startframe:\n",
    "            feats.append(signal[:, int(asf):int(asf)+max_audio])\n",
    "        feat = torch.stack(feats,0) # shape: [n_eval_cuts, n_channels, time]\n",
    "\n",
    "    return feat # shape: [n_eval_cuts, n_channels, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, id2, label = valid_dataset[0]\n",
    "wav_id1, _, _, _ = id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = valid_transform(sample_duration,  wav_id1, sample_rate, n_eval_cuts=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32000])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=image_T.transform, \n",
    "                            audio_transform=valid_transform) #audio_T.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3616158) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    180\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m     gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 3616158) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m valid_dataloader:\n\u001b[1;32m      3\u001b[0m     id1, id2, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      5\u001b[0m     wav_id1, rgb_id1, thr_id1, _ \u001b[38;5;241m=\u001b[39m id1\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3616158) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for batch in valid_dataloader:\n",
    "\n",
    "    id1, id2, labels = batch\n",
    "\n",
    "    wav_id1, rgb_id1, thr_id1, _ = id1\n",
    "    wav_id2, rgb_id2, thr_id2, _ = id2\n",
    "\n",
    "    if modality == \"rgb\":\n",
    "        data_id1 = rgb_id1.to(device)\n",
    "        data_id2 = rgb_id2.to(device)\n",
    "\n",
    "    elif modality == \"thr\":\n",
    "        data_id1 = thr_id1.to(device)\n",
    "        data_id2 = thr_id2.to(device)\n",
    "\n",
    "    elif modality == \"wav\":\n",
    "        data_id1 = wav_id1.to(device)\n",
    "        data_id2 = wav_id2.to(device)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 83149])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_id1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with torch.no_grad():\n",
    "        id1_out = model(data_id1)\n",
    "        id2_out = model(data_id2)\n",
    "\n",
    "        cos_sim = F.cosine_similarity(id1_out, id2_out, dim=1)\n",
    "        eer, scores = EER_(cos_sim, labels)\n",
    "        accuracy = accuracy_(labels, scores)\n",
    "\n",
    "        total_eer += eer\n",
    "        total_accuracy += accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST (Audio Spectrogram Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor()\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.dense.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAvLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                library=\"pytorch\", \n",
    "                pretrained_weights=True, \n",
    "                fine_tune=False, \n",
    "                embedding_size=128, \n",
    "                modality = \"rgb\",\n",
    "                model_name = \"resnet34\",\n",
    "                pool=\"default\"):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        if modality == \"wav\":\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "    \n",
    "        if library == \"pytorch\":\n",
    "            if model_name == \"resnet34\":\n",
    "                if pretrained_weights:\n",
    "                    weights = models.ResNet34_Weights.DEFAULT\n",
    "                else:\n",
    "                    weights = None\n",
    "\n",
    "                self.model = models.resnet34(weights=weights)\n",
    "                if modality == \"wav\":\n",
    "                    self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "                self.model.fc = nn.Linear(self.model.fc.in_features, embedding_size)\n",
    "                \n",
    "                if pool == \"SAP\":\n",
    "                    self.model.avgpool = SelfAttentivePool2d(self.model.fc.in_features)\n",
    "\n",
    "            if fine_tune:\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "                self.model.fc.weight.requires_grad = True\n",
    "                self.model.fc.bias.requires_grad = True\n",
    "\n",
    "        elif library == \"timm\":\n",
    "            self.model = timm.create_model(model_name, pretrained=pretrained_weights, num_classes=embedding_size, in_chans=in_channels)\n",
    "\n",
    "            if pool == \"SAP\":\n",
    "                self.model.global_pool = SelfAttentivePool2d()\n",
    "            if fine_tune:\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                self.model.get_classifier().weight.requires_grad = True\n",
    "                self.model.get_classifier().bias.requires_grad = True\n",
    "        \n",
    "        elif library == \"huggingface\":\n",
    "            if model_name == \"WavLM\":\n",
    "                self.model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n",
    "                self.model.classifier = nn.Linear(self.model.classifier.in_features, embedding_size)\n",
    "\n",
    "                if fine_tune:\n",
    "                    for param in self.model.parameters():\n",
    "                        param.requires_grad = True\n",
    "                else:\n",
    "                    for param in self.model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                    \n",
    "                    self.model.classifier.weight.requires_grad = True\n",
    "                    self.model.classifier.bias.requires_grad = True\n",
    "                \n",
    "            elif model_name == \"AST\":\n",
    "                self.model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "                self.model.classifier.dense = nn.Linear(self.model.classifier.dense.in_features, embedding_size)\n",
    "\n",
    "                if fine_tune:\n",
    "                    for param in self.model.parameters():\n",
    "                        param.requires_grad = True\n",
    "                else:\n",
    "                    for param in self.model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                    \n",
    "                    self.model.classifier.dense.weight.requires_grad = True\n",
    "                    self.model.classifier.dense.bias.requires_grad = True\n",
    "\n",
    "        self.library = library\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.library == \"huggingface\":\n",
    "            if self.model_name == \"WavLM\":\n",
    "                x = self.model(x).embeddings\n",
    "            elif self.model_name == \"AST\":\n",
    "                x = self.model(x).logits\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_Transforms:\n",
    "    def __init__(self, \n",
    "                sample_rate,\n",
    "                sample_duration, # seconds\n",
    "                n_fft, # from Korean code\n",
    "                win_length,\n",
    "                hop_length,\n",
    "                window_fn,\n",
    "                n_mels,\n",
    "                model_name=None):\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_duration = sample_duration\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.window_fn = window_fn\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        if model_name == \"WavLM\":\n",
    "            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "        elif model_name == \"AST\":\n",
    "            self.feature_extractor = ASTFeatureExtractor()\n",
    "\n",
    "    def basic_transform(self, signal, sample_rate):\n",
    "\n",
    "        # stereo --> mono\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        \n",
    "        # sample_rate --> 16000\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        # normalize duration --> 3 seconds (mean duration in dataset)\n",
    "        sample_length_signal = self.sample_duration * self.sample_rate # sample length of the audio signal\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < sample_length_signal:\n",
    "            num_missing_points = int(sample_length_signal - length_signal)\n",
    "            dim_padding = (0, num_missing_points) # (left_pad, right_pad)\n",
    "            # ex: dim_padding = (1,2) --> [1,1,1] -> [0,1,1,1,0,0]\n",
    "            signal = torch.nn.functional.pad(signal, dim_padding)\n",
    "        elif length_signal > sample_length_signal:\n",
    "            middle_of_the_signal = length_signal // 2\n",
    "            left_edge = int(middle_of_the_signal - sample_length_signal // 2)\n",
    "            right_edge = int(middle_of_the_signal + sample_length_signal // 2)\n",
    "            signal = signal[:,left_edge:right_edge]\n",
    "            \n",
    "        return signal\n",
    "\n",
    "    def HF_transform(self, signal, sample_rate):\n",
    "        signal = self.basic_transform(signal, sample_rate)\n",
    "        signal = signal.squeeze()\n",
    "        inputs = self.feature_extractor(signal, sampling_rate=sample_rate, padding=True, return_tensors=\"pt\")\n",
    "        return inputs.input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "library=\"huggingface\"\n",
    "modality = \"wav\"\n",
    "model_name = \"AST\"\n",
    "pool=\"default\"\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_T = Audio_Transforms(sample_rate=16000,\n",
    "                            sample_duration=3, # seconds\n",
    "                            n_fft=512, # from Korean code\n",
    "                            win_length=400,\n",
    "                            hop_length=160,\n",
    "                            window_fn=torch.hamming_window,\n",
    "                            n_mels=40,\n",
    "                            model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ANNOTATIONS_FILE = \"/workdir/Speaker_Verification_version_1.0/Speaker-Verification/annotations_file_short_SF.csv\"\n",
    "PATH2DATASET = \"/workdir/sf_pv\"\n",
    "DATASET_DIR = f'{PATH2DATASET}/data_v2'\n",
    "\n",
    "train_dataset = SpeakingFacesDataset(ANNOTATIONS_FILE,DATASET_DIR,'train',\n",
    "                                    image_transform=None, \n",
    "                                    audio_transform=audio_T.HF_transform)\n",
    "\n",
    "valid_dataset = ValidDataset(PATH2DATASET,'valid',\n",
    "                            image_transform=None, \n",
    "                            audio_transform=audio_T.HF_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 128])\n",
      "torch.Size([1, 1024, 128])\n",
      "torch.Size([1, 1024, 128])\n",
      "torch.Size([1, 1024, 128])\n",
      "torch.Size([1, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    wav, _, _, _ = train_dataset[i]\n",
    "    print(wav.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler\n",
    "train_sampler = ProtoSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 10,\n",
    "                                    n_ways = 3, # n_way\n",
    "                                    n_support = 1, # n_shots\n",
    "                                    n_query = 1)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=4, pin_memory=True\n",
    "                          )\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4, \n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m total_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# data_wav, data_rgb, data_thr, label\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         _,rgb, _, _ \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;66;03m# we do not use labels from dataset\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(train_dataloader, desc=f'Train (epoch = {1})', leave=False)  \n",
    "\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "for batch in pbar:\n",
    "\n",
    "    if modality == \"rgb\":\n",
    "        # data_wav, data_rgb, data_thr, label\n",
    "        _,rgb, _, _ = batch # we do not use labels from dataset\n",
    "        data = rgb.to(device)\n",
    "    elif modality == \"thr\":\n",
    "        # data_wav, data_rgb, data_thr, label\n",
    "        _,_, thr, _ = batch # we do not use labels from dataset\n",
    "        data = thr.to(device)\n",
    "    elif modality == \"wav\":\n",
    "        wav, _, _, _ = batch # we do not use labels from dataset\n",
    "        data = wav.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type='squared_euclidean')\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workdir/results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchern\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workdir/Speaker_Verification_version_1.0/Speaker-Verification/speaker_verification/train.py:42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, valid_dataloader, train_sampler, criterion, optimizer, scheduler, device, num_epochs, save_dir, exp_name, modality, wandb)\u001b[0m\n\u001b[1;32m     39\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     40\u001b[0m start_train \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> 42\u001b[0m model, train_loss, train_acc \u001b[39m=\u001b[39m train_singe_epoch(model, \n\u001b[1;32m     43\u001b[0m                           train_dataloader,\n\u001b[1;32m     44\u001b[0m                           epoch, \n\u001b[1;32m     45\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_ways, \n\u001b[1;32m     46\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_shots, \n\u001b[1;32m     47\u001b[0m                           train_sampler\u001b[39m.\u001b[39;49mn_query,\n\u001b[1;32m     48\u001b[0m                           criterion,\n\u001b[1;32m     49\u001b[0m                           optimizer,\n\u001b[1;32m     50\u001b[0m                           device,\n\u001b[1;32m     51\u001b[0m                           modality)\n\u001b[1;32m     53\u001b[0m end_train \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     54\u001b[0m logs[\u001b[39m'\u001b[39m\u001b[39mtrain_time_min\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend((end_train \u001b[39m-\u001b[39m start_train)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m)\n",
      "File \u001b[0;32m/workdir/Speaker_Verification_version_1.0/Speaker-Verification/speaker_verification/train.py:119\u001b[0m, in \u001b[0;36mtrain_singe_epoch\u001b[0;34m(model, train_dataloader, epoch, n_ways, n_shots, n_query, criterion, optimizer, device, modality)\u001b[0m\n\u001b[1;32m    117\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    118\u001b[0m total_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    122\u001b[0m         \u001b[39m# data_wav, data_rgb, data_thr, label\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         _,rgb, _, _ \u001b[39m=\u001b[39m batch \u001b[39m# we do not use labels from dataset\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model,\n",
    "                    train_dataloader, \n",
    "                    valid_dataloader,\n",
    "                    train_sampler,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    num_epochs=1,\n",
    "                    save_dir=\"/workdir/results\",\n",
    "                    exp_name=\"chern\",\n",
    "                    modality='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soundfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m path2wav3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workdir/sf_pv/data_v2/sub_1/12/wav/673.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m path2wav4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workdir/sf_pv/data_v2/sub_2/11/wav/38.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m data_wav_id1_1, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43msoundfile\u001b[49m\u001b[38;5;241m.\u001b[39mread(path2wav1) \n\u001b[1;32m      6\u001b[0m data_wav_id1_2, sample_rate \u001b[38;5;241m=\u001b[39m soundfile\u001b[38;5;241m.\u001b[39mread(path2wav2) \n\u001b[1;32m      7\u001b[0m data_wav_id1_3, sample_rate \u001b[38;5;241m=\u001b[39m soundfile\u001b[38;5;241m.\u001b[39mread(path2wav3) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'soundfile' is not defined"
     ]
    }
   ],
   "source": [
    "path2wav1 = \"/workdir/sf_pv/data_v2/sub_1/11/wav/574.wav\"\n",
    "path2wav2 = \"/workdir/sf_pv/data_v2/sub_1/11/wav/603.wav\"\n",
    "path2wav3 = \"/workdir/sf_pv/data_v2/sub_1/12/wav/673.wav\"\n",
    "path2wav4 = \"/workdir/sf_pv/data_v2/sub_2/11/wav/38.wav\"\n",
    "data_wav_id1_1, sample_rate = soundfile.read(path2wav1) \n",
    "data_wav_id1_2, sample_rate = soundfile.read(path2wav2) \n",
    "data_wav_id1_3, sample_rate = soundfile.read(path2wav3) \n",
    "data_wav_id2, sample_rate = soundfile.read(path2wav4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchaudio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_wav, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241m.\u001b[39mload(path2wav4)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchaudio' is not defined"
     ]
    }
   ],
   "source": [
    "data_wav, sr = torchaudio.load(path2wav4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60959])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wav = feature_extractor(data_wav.squeeze(), sampling_rate=16000, padding=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60959])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wav.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute attention masks and normalize the waveform if needed\n",
    "inputs1 = feature_extractor(data_wav_id1_1, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "inputs2 = feature_extractor(data_wav_id1_2, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "inputs3 = feature_extractor(data_wav_id1_3, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "inputs4 = feature_extractor(data_wav_id2, sampling_rate=16000, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ast_transform(data_wav, sample_rate):\n",
    "    data_wav = data_wav.squeeze()\n",
    "    inputs = feature_extractor(data_wav, sampling_rate=sample_rate, padding=True, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**data_wav).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model(**inputs1).embeddings\n",
    "output2 = model(**inputs2).embeddings\n",
    "output3 = model(**inputs3).embeddings\n",
    "output4 = model(**inputs4).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = torch.nn.functional.normalize(output1, dim=-1).cpu()\n",
    "emb2 = torch.nn.functional.normalize(output2, dim=-1).cpu()\n",
    "emb3 = torch.nn.functional.normalize(output3, dim=-1).cpu()\n",
    "emb4 = torch.nn.functional.normalize(output4, dim=-1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8827], grad_fn=<SumBackward1>)\n",
      "tensor([0.8955], grad_fn=<SumBackward1>)\n",
      "tensor([0.8481], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# the resulting embeddings can be used for cosine similarity-based retrieval\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarity = cosine_sim(embeddings1, embeddings2)\n",
    "threshold = 0.86  # the optimal threshold is dataset-dependent\n",
    "print(cosine_sim(emb1, emb2))\n",
    "print(cosine_sim(emb1, emb3))\n",
    "print(cosine_sim(emb1, emb4))\n",
    "if similarity < threshold:\n",
    "    print(\"Speakers are not the same!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
