{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/github\n"
     ]
    }
   ],
   "source": [
    "cd /workdir/github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "torch.set_num_threads(1)\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model\n",
    "from speaker_verification.dataset import SpeakingFacesDataset\n",
    "from speaker_verification.dataset import TrainDataset\n",
    "from speaker_verification.dataset import ValidDataset\n",
    "from speaker_verification.sampler import ProtoSampler\n",
    "from speaker_verification.loss import PrototypicalLoss\n",
    "from speaker_verification.train import train_model\n",
    "\n",
    "from speaker_verification.metrics import EER_\n",
    "from speaker_verification.metrics import accuracy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "annotations_file = \"/workdir/github/annotations_file_short_joint_cleaned.csv\"\n",
    "dataset_type = \"SF\"\n",
    "\n",
    "if dataset_type == \"SF\":\n",
    "    path_to_train_dataset = f\"/workdir/sf_pv/data_v2\"\n",
    "    path_to_valid_dataset = \"/workdir/sf_pv\"\n",
    "    path_to_valid_list = \"/workdir/sf_pv/metadata/valid_list_v2.txt\"\n",
    "elif dataset_type == \"VX2\":\n",
    "    path_to_train_dataset=\"/workdir/VoxCeleb2/dev\"\n",
    "    path_to_valid_list=\"/workdir/VoxCeleb1/metadata/test_list_vc_v2.txt\"\n",
    "    path_to_valid_dataset=\"/workdir/VoxCeleb1/test\"\n",
    "\n",
    "    \n",
    "data_type = ['rgb']\n",
    "\n",
    "\n",
    "# model\n",
    "library = \"timm\"\n",
    "model_name = \"resnet34\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "\n",
    "# transform\n",
    "audio_T = None\n",
    "image_T = None\n",
    "\n",
    "# sampler\n",
    "n_batch=10\n",
    "n_ways=2\n",
    "n_support=1\n",
    "n_query=1\n",
    "\n",
    "# loss\n",
    "dist_type='squared_euclidean'\n",
    "\n",
    "# train\n",
    "num_epochs=1\n",
    "save_dir='/workdir/results'\n",
    "exp_name='chern'\n",
    "wandb=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'wav' in data_type:\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "    audio_T = audio_T.transform\n",
    "\n",
    "if 'rgb' in data_type or 'thr' in data_type:\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    image_T = image_T.transform         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = TrainDataset(annotations_file=annotations_file,\n",
    "                            path_to_train_dataset=path_to_train_dataset,\n",
    "                            train_type='train',\n",
    "                            image_transform=image_T, \n",
    "                            audio_transform=audio_T,\n",
    "                            data_type=data_type,\n",
    "                            dataset_type=dataset_type)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=128,\n",
    "                        num_workers=4)\n",
    "\n",
    "valid_dataset = ValidDataset(path_to_valid_dataset=path_to_valid_dataset,\n",
    "                             path_to_valid_list=path_to_valid_list,\n",
    "                             data_type=data_type,\n",
    "                             dataset_type=dataset_type,\n",
    "                             image_transform=image_T, \n",
    "                             audio_transform=audio_T)\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "n_classes = len(np.unique(train_dataset.labels))\n",
    "classification_layer = torch.nn.Linear(embedding_size, n_classes)\n",
    "\n",
    "model = torch.nn.Sequential()\n",
    "model.add_module('pretrained_model', pretrained_model)\n",
    "model.add_module('classification_layer', classification_layer)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_singe_epoch(model,\n",
    "                      train_dataloader, \n",
    "                      epoch,\n",
    "                      optimizer,\n",
    "                      device):\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, desc=f'Train (epoch = {epoch})', leave=False)  \n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch in pbar:\n",
    "\n",
    "        data, label = batch\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        data = model(data)\n",
    "        loss = F.cross_entropy(data, label)\n",
    "        pred = torch.argmax(F.softmax(data,dim=1), dim=1)\n",
    "        accuracy = (pred == label).sum()/len(label) * 100\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    avg_acc = total_acc / len(train_dataloader)\n",
    "\n",
    "    print()\n",
    "    print(f\"Average train loss: {avg_loss}\")\n",
    "    print(f\"Average train accuracy: {avg_acc}\")\n",
    "\n",
    "    return model, avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86e86d49b864fbe974e55d1d51ca760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train (epoch = 1):   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 0.9223150528252941\n",
      "Average train accuracy: 79.82862603174497\n"
     ]
    }
   ],
   "source": [
    "model, avg_loss, avg_acc = train_singe_epoch(model,\n",
    "                      train_dataloader, \n",
    "                      1,\n",
    "                      optimizer,\n",
    "                      device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_dataloader:\n",
    "    id1, id2, labels = batch\n",
    "    if len(data_type) == 1:\n",
    "        data_id1, _ = id1\n",
    "        data_id2, _ = id2\n",
    "\n",
    "        data_id1 = data_id1.to(device)\n",
    "        data_id2 = data_id2.to(device)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    id1_out = model.pretrained_model(data_id1)\n",
    "    id2_out = model.pretrained_model(data_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_epoch(model,\n",
    "                        val_dataloader,\n",
    "                        epoch, \n",
    "                        device,\n",
    "                        data_type):\n",
    "    model.eval()\n",
    "    total_eer = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    pbar = tqdm(val_dataloader, desc=f'Eval (epoch = {epoch})')\n",
    "\n",
    "    for batch in pbar:\n",
    "\n",
    "        data_type = sorted(data_type)\n",
    "        id1, id2, labels = batch\n",
    "\n",
    "        if len(data_type) == 1:\n",
    "            data_id1, _ = id1\n",
    "            data_id2, _ = id2\n",
    "\n",
    "            data_id1 = data_id1.to(device)\n",
    "            data_id2 = data_id2.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # id1_out = model(data_id1)\n",
    "            # id2_out = model(data_id2)\n",
    "\n",
    "            id1_out = model.pretrained_model(data_id1)\n",
    "            id2_out = model.pretrained_model(data_id2)\n",
    "\n",
    "            cos_sim = F.cosine_similarity(id1_out, id2_out, dim=1)\n",
    "            eer, scores = EER_(cos_sim, labels)\n",
    "            accuracy = accuracy_(labels, scores)\n",
    "\n",
    "            total_eer += eer\n",
    "            total_accuracy += accuracy\n",
    "    \n",
    "    avg_eer = total_eer / len(val_dataloader)\n",
    "    print(\"\\nAverage val eer: {}\".format(avg_eer))\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(val_dataloader)\n",
    "    print(\"\\nAverage val accuracy: {}\".format(avg_accuracy))\n",
    "\n",
    "    return model, avg_eer, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526b3297f05247b59cfcbbeb1d176011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (epoch = 1):   0%|          | 0/297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 8.192791005291005\n",
      "\n",
      "Average val accuracy: 91.44645863395863\n"
     ]
    }
   ],
   "source": [
    "model, avg_eer, avg_accuracy = evaluate_single_epoch(model,\n",
    "                        valid_dataloader,\n",
    "                        1, \n",
    "                        device,\n",
    "                        data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_dataloader, \n",
    "                valid_dataloader):\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        model, train_loss, train_acc = train_singe_epoch(model, \n",
    "                                  train_dataloader,\n",
    "                                  epoch,\n",
    "                                  optimizer,\n",
    "                                  device)\n",
    "        \n",
    "        model, val_eer, val_acc = evaluate_single_epoch(model, \n",
    "                                  valid_dataloader,\n",
    "                                  epoch,\n",
    "                                  device,\n",
    "                                  data_type)\n",
    "\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0348527746fc42719317b8e25c0343c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629cd50fdd02406ba0a4a31b6512007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train (epoch = 0):   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 0.0807525069797284\n",
      "Average train accuracy: 97.8167808219178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b987e5a55aa4961b9c20c3ef07bdf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (epoch = 0):   0%|          | 0/297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val eer: 6.672378547378548\n",
      "\n",
      "Average val accuracy: 92.86277958152958\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, \n",
    "                train_dataloader, \n",
    "                valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
