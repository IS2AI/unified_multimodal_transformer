{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/github\n"
     ]
    }
   ],
   "source": [
    "cd /workdir/github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_verification.transforms import Audio_Transforms\n",
    "from speaker_verification.transforms import Image_Transforms\n",
    "from speaker_verification.models import Model\n",
    "from speaker_verification.dataset import SpeakingFacesDataset\n",
    "from speaker_verification.dataset import ValidDataset\n",
    "from speaker_verification.sampler import ProtoSampler\n",
    "from speaker_verification.loss import PrototypicalLoss\n",
    "from speaker_verification.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "from transformers import WavLMForXVector\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pretrained_weights : bool, default = \"True\"\n",
    "            Ways of weights initialization. \n",
    "            If \"False\", it means random initialization and no pretrained weights,\n",
    "            If \"True\" it means resnet34 pretrained weights are used.\n",
    "\n",
    "        fine_tune: bool, default = \"False\"\n",
    "            Allows to choose between two types of transfer learning: fine tuning and feature extraction.\n",
    "            For more details of the description of each mode, \n",
    "            read https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "        embedding_size: int, default = 128\n",
    "            Size of the embedding of the last layer\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                library,\n",
    "                pretrained_weights,\n",
    "                fine_tune,\n",
    "                embedding_size,\n",
    "                model_name,\n",
    "                pool, \n",
    "                data_type):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.library = library\n",
    "        self.pretrained_weights = pretrained_weights\n",
    "        self.fine_tune = fine_tune\n",
    "        self.embedding_size = embedding_size\n",
    "        self.model_name = model_name\n",
    "        self.pool = pool\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        if len(data_type) == 1:\n",
    "\n",
    "            if data_type[0] == \"wav\":\n",
    "                print(\"wav data type\")\n",
    "                self.model = self.wav_model()\n",
    "            elif data_type[0] == \"rgb\":\n",
    "                print(\"rgb data type\")\n",
    "                self.model = self.image_model()\n",
    "            elif data_type[0] == \"thr\":\n",
    "                print(\"thr data type\")\n",
    "                self.model = self.image_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.library == \"huggingface\":\n",
    "            x = self.model(x).logits\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def image_model(self):\n",
    "\n",
    "        if self.library == \"huggingface\":\n",
    "            print(\"HuggingFace model is used.\")\n",
    "            pass\n",
    "        elif self.library == \"pytorch\":\n",
    "            print(\"pytorch model is used.\")\n",
    "            model = self.pytorch_model(in_channels = 3)\n",
    "        elif self.library == \"timm\":\n",
    "            print(\"timm model is used.\")\n",
    "            model = self.timm_model(in_channels = 3)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def wav_model(self):\n",
    "\n",
    "        if self.library == \"huggingface\":\n",
    "            print(\"HuggingFace model is used.\")\n",
    "            model = self.huggingface_model()\n",
    "        elif self.library == \"pytorch\":\n",
    "            print(\"pytorch model is used.\")\n",
    "            model = self.pytorch_model(in_channels = 1)\n",
    "        elif self.library == \"timm\":\n",
    "            print(\"timm model is used.\")\n",
    "            model = self.timm_model(in_channels = 1)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def huggingface_model(self):\n",
    "        if self.model_name == \"WavLM\":\n",
    "            print(\"WavLM model is used.\")\n",
    "            model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n",
    "            model.classifier = nn.Linear(model.classifier.in_features, self.embedding_size)\n",
    "\n",
    "            if self.fine_tune:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "                model.classifier.weight.requires_grad = True\n",
    "                model.classifier.bias.requires_grad = True\n",
    "        elif self.model_name == \"AST\":\n",
    "            print(\"AST model is used.\")\n",
    "            model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "            model.classifier.dense = nn.Linear(model.classifier.dense.in_features, self.embedding_size)\n",
    "\n",
    "            if self.fine_tune:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "                model.classifier.dense.weight.requires_grad = True\n",
    "                model.classifier.dense.bias.requires_grad = True\n",
    "\n",
    "        return model\n",
    "\n",
    "    def timm_model(self, in_channels):\n",
    "        model = timm.create_model(self.model_name, pretrained=self.pretrained_weights, num_classes=self.embedding_size, in_chans=in_channels)\n",
    "\n",
    "        if self.pool == \"SAP\":\n",
    "            model.global_pool = SelfAttentivePool2d()\n",
    "        if self.fine_tune:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.get_classifier().weight.requires_grad = True\n",
    "            model.get_classifier().bias.requires_grad = True\n",
    "\n",
    "        return model\n",
    "\n",
    "    def pytorch_model(self, in_channels):\n",
    "        if self.model_name == \"resnet34\":\n",
    "            if self.pretrained_weights:\n",
    "                weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "            else:\n",
    "                weights = None\n",
    "\n",
    "            model = torchvision.models.resnet34(weights=weights)\n",
    "            \n",
    "            model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            model.fc = nn.Linear(model.fc.in_features, self.embedding_size)\n",
    "            \n",
    "            if self.pool == \"SAP\":\n",
    "                model.avgpool = SelfAttentivePool2d(model.fc.in_features)\n",
    "\n",
    "        if self.fine_tune:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            model.fc.weight.requires_grad = True\n",
    "            model.fc.bias.requires_grad = True\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check wav models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check huggingface models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"huggingface\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"AST\"\n",
    "pool=None\n",
    "data_type=[\"wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "HuggingFace model is used.\n",
      "AST model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size, \n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "input = torch.rand(45789)\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "input = feature_extractor(input, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "input = input.input_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"huggingface\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"WavLM\"\n",
    "pool=None\n",
    "data_type=[\"wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "HuggingFace model is used.\n",
      "WavLM model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "input = torch.rand(45789)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor()\n",
    "input = feature_extractor(input, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "input = input.input_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=None\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "input = torch.rand((1,45789))\n",
    "\n",
    "to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                window_fn=torch.hamming_window,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "            \n",
    "input = to_MelSpectrogram(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 287])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet34 + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=\"SAP\"\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "input = torch.rand((1,45789))\n",
    "\n",
    "to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                window_fn=torch.hamming_window,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "            \n",
    "input = to_MelSpectrogram(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check vit_base_patch16_224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"vit_base_patch16_224\"\n",
    "pool=None\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=3 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "input = torch.rand((1,45789))\n",
    "\n",
    "to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                window_fn=torch.hamming_window,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "\n",
    "transform=T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(size=256, interpolation=T.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "    T.CenterCrop(size=(224, 224)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=torch.tensor([0.4850]), std=torch.tensor([0.2290]))\n",
    "])\n",
    "            \n",
    "input = to_MelSpectrogram(input)\n",
    "# input = input.repeat(3, 1, 1)\n",
    "input = transform(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check vit + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"vit_base_patch16_224\"\n",
    "pool=\"SAP\"\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=3 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "to_PILImage = T.ToPILImage()\n",
    "\n",
    "input = torch.rand((1,45789))\n",
    "\n",
    "to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                window_fn=torch.hamming_window,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "\n",
    "transform=T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(size=256, interpolation=T.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "    T.CenterCrop(size=(224, 224)),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=torch.tensor([0.4850]), std=torch.tensor([0.2290]))\n",
    "])\n",
    "            \n",
    "input = to_MelSpectrogram(input)\n",
    "# input = input.repeat(3, 1, 1)\n",
    "input = transform(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"pytorch\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=None\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "# audio\n",
    "sample_rate=16000\n",
    "sample_duration=3 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "input = torch.rand((1,45789))\n",
    "\n",
    "to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                window_fn=torch.hamming_window,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "            \n",
    "input = to_MelSpectrogram(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=None\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((128,128)),\n",
    "            torchvision.transforms.ToTensor(), \n",
    "        ])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=\"SAP\"\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((128,128)),\n",
    "            torchvision.transforms.ToTensor(), \n",
    "        ])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"vit_base_patch16_224\"\n",
    "pool=None\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize(size=256, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "    torchvision.transforms.CenterCrop(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=torch.tensor([0.4850]), std=torch.tensor([0.2290]))\n",
    "])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check vit + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"timm\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"vit_base_patch16_224\"\n",
    "pool=\"SAP\"\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize(size=256, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "    torchvision.transforms.CenterCrop(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=torch.tensor([0.4850]), std=torch.tensor([0.2290]))\n",
    "])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"pytorch\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=None\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((128,128)),\n",
    "            torchvision.transforms.ToTensor(), \n",
    "        ])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check resnet + SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"pytorch\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "model_name=\"resnet34\"\n",
    "pool=\"SAP\"\n",
    "data_type=[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "pytorch model is used.\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize((128,128)),\n",
    "            torchvision.transforms.ToTensor(), \n",
    "        ])\n",
    "\n",
    "input = transform_image(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "class Audio_Transforms:\n",
    "    def __init__(self, \n",
    "                sample_rate,\n",
    "                sample_duration, # seconds\n",
    "                n_fft, # from Korean code\n",
    "                win_length,\n",
    "                hop_length,\n",
    "                window_fn,\n",
    "                n_mels,\n",
    "                model_name, \n",
    "                library\n",
    "                ):\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_duration = sample_duration\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.window_fn = window_fn\n",
    "        self.n_mels = n_mels\n",
    "        self.model_name = model_name\n",
    "        self.library = library\n",
    "\n",
    "        if self.library == \"huggingface\":\n",
    "            self.huggingface_init()\n",
    "        elif self.library == \"timm\":\n",
    "            self.timm_init()\n",
    "        elif self.library == \"pytorch\":\n",
    "            self.pytorch_init()\n",
    "\n",
    "    def huggingface_init(self):\n",
    "        if self.model_name == \"WavLM\":\n",
    "            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n",
    "        elif self.model_name == \"AST\":\n",
    "            self.feature_extractor = ASTFeatureExtractor()\n",
    "\n",
    "    def timm_init(self):\n",
    "        self.to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_fft=self.n_fft,\n",
    "                win_length=self.win_length,\n",
    "                hop_length=self.hop_length,\n",
    "                window_fn=self.window_fn,\n",
    "                n_mels=self.n_mels\n",
    "            )\n",
    "\n",
    "        if self.model_name == \"vit_base_patch16_224\":\n",
    "            # n_channels = 1\n",
    "            self.vit_transform=T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize(size=256, interpolation=T.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "                T.CenterCrop(size=(224, 224)),\n",
    "                T.ToTensor(),\n",
    "                # T.Normalize(mean=torch.tensor([0.4850]), std=torch.tensor([0.2290]))\n",
    "            ])\n",
    "        \n",
    "\n",
    "    def pytorch_init(self):\n",
    "        self.to_MelSpectrogram =  torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_fft=self.n_fft,\n",
    "                win_length=self.win_length,\n",
    "                hop_length=self.hop_length,\n",
    "                window_fn=self.window_fn,\n",
    "                n_mels=self.n_mels\n",
    "            )\n",
    "\n",
    "    # MAIN TRANSFORM FUNCTION\n",
    "    def transform(self, signal, sample_rate):\n",
    "\n",
    "        signal = self.basic_transform(signal, sample_rate)\n",
    "\n",
    "        if self.library == \"huggingface\":\n",
    "            inputs = self.huggingface_transform(signal)\n",
    "        elif self.library == \"timm\":\n",
    "            inputs = self.timm_transform(signal)\n",
    "        elif self.library == \"pytorch\":\n",
    "            inputs = self.pytorch_transform(signal)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def basic_transform(self, signal, sample_rate):\n",
    "\n",
    "        # stereo --> mono\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        \n",
    "        # sample_rate --> 16000\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        # normalize duration --> 3 seconds (mean duration in dataset)\n",
    "        sample_length_signal = self.sample_duration * self.sample_rate # sample length of the audio signal\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < sample_length_signal:\n",
    "            num_missing_points = int(sample_length_signal - length_signal)\n",
    "            dim_padding = (0, num_missing_points) # (left_pad, right_pad)\n",
    "            # ex: dim_padding = (1,2) --> [1,1,1] -> [0,1,1,1,0,0]\n",
    "            signal = torch.nn.functional.pad(signal, dim_padding)\n",
    "        elif length_signal > sample_length_signal:\n",
    "            middle_of_the_signal = length_signal // 2\n",
    "            left_edge = int(middle_of_the_signal - sample_length_signal // 2)\n",
    "            right_edge = int(middle_of_the_signal + sample_length_signal // 2)\n",
    "            signal = signal[:,left_edge:right_edge]\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def huggingface_transform(self, audio):\n",
    "        input = audio.squeeze()\n",
    "        input = self.feature_extractor(input, sampling_rate=self.sample_rate, padding=True, return_tensors=\"pt\")\n",
    "        input = input.input_values.squeeze()\n",
    "        return input\n",
    "\n",
    "    def timm_transform(self, audio):\n",
    "        input = self.to_MelSpectrogram(audio)\n",
    "        if self.model_name == \"vit_base_patch16_224\":\n",
    "            # input = input.repeat(3, 1, 1)\n",
    "            input = self.vit_transform(input)\n",
    "        return input\n",
    "\n",
    "    def pytorch_transform(self, audio):\n",
    "        input = self.to_MelSpectrogram(audio)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.rand((1,45789))\n",
    "\n",
    "path2wav = \"/workdir/sf_pv/data_v2/sub_1/11/wav/574.wav\"\n",
    "audio, sample_rate = torchaudio.load(path2wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 66834])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"AST\", \"WavLM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "HuggingFace model is used.\n",
      "AST model is used.\n",
      "torch.Size([1, 128])\n",
      "wav data type\n",
      "HuggingFace model is used.\n",
      "WavLM model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    library = \"huggingface\"\n",
    "\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "\n",
    "    # model params\n",
    "    pretrained_weights=True\n",
    "    fine_tune=True\n",
    "    embedding_size=128\n",
    "    pool=None\n",
    "    data_type=[\"wav\"]\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "\n",
    "    model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    input = audio_T.transform(audio, sample_rate)\n",
    "    out = model(input.unsqueeze(dim=0).to(device))\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check timm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['resnet34', 'vit_base_patch16_224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "timm model is used.\n",
      "torch.Size([1, 128])\n",
      "wav data type\n",
      "timm model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    library = \"timm\"\n",
    "    \n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "\n",
    "    # model params\n",
    "    pretrained_weights=True\n",
    "    fine_tune=True\n",
    "    embedding_size=128\n",
    "    pool=None\n",
    "    data_type=[\"wav\"]\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "\n",
    "    model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    input = audio_T.transform(audio, sample_rate)\n",
    "    out = model(input.unsqueeze(dim=0).to(device))\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "pytorch model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "library = \"pytorch\"\n",
    "model_name = 'resnet34'\n",
    "    \n",
    "# audio transform params\n",
    "sample_rate=16000\n",
    "sample_duration=2 # seconds\n",
    "n_fft=512 # from Korean code\n",
    "win_length=400\n",
    "hop_length=160\n",
    "window_fn=torch.hamming_window\n",
    "n_mels=40\n",
    "\n",
    "# model params\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "data_type=[\"wav\"]\n",
    "\n",
    "audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                            sample_duration=sample_duration, # seconds\n",
    "                            n_fft=n_fft, # from Korean code\n",
    "                            win_length=win_length,\n",
    "                            hop_length=hop_length,\n",
    "                            window_fn=torch.hamming_window,\n",
    "                            n_mels=n_mels,\n",
    "                            model_name=model_name,\n",
    "                            library=library)\n",
    "\n",
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "input = audio_T.transform(audio, sample_rate)\n",
    "out = model(input.unsqueeze(dim=0).to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Transforms:\n",
    "    def __init__(self, \n",
    "                 library,\n",
    "                 model_name):\n",
    "\n",
    "        self.library = library\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if self.library == \"huggingface\":\n",
    "            pass\n",
    "        elif self.library == \"timm\":\n",
    "            self.timm_init()\n",
    "        elif self.library == \"pytorch\":\n",
    "            self.pytorch_init()\n",
    "\n",
    "    def timm_init(self):\n",
    "        if self.model_name == \"vit_base_patch16_224\":\n",
    "            # n_channels = 3\n",
    "            self.transform_image=T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize(size=256, interpolation=T.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "                T.CenterCrop(size=(224, 224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "            ]) \n",
    "        else:\n",
    "            self.transform_image = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((128,128)),\n",
    "                T.ToTensor(), \n",
    "            ])\n",
    "        \n",
    "    def pytorch_init(self):\n",
    "        self.transform_image = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((128,128)),\n",
    "                T.ToTensor(), \n",
    "            ])\n",
    "\n",
    "    def transform(self, image):\n",
    "        return self.transform_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.random.rand(175, 130, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['resnet34', 'vit_base_patch16_224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n",
      "torch.Size([1, 128])\n",
      "rgb data type\n",
      "timm model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    library = \"timm\"\n",
    "    \n",
    "    # model params\n",
    "    pretrained_weights=True\n",
    "    fine_tune=True\n",
    "    embedding_size=128\n",
    "    pool=None\n",
    "    data_type=[\"rgb\"]\n",
    "\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    model = Model(library=library, \n",
    "                pretrained_weights=pretrained_weights, \n",
    "                fine_tune=fine_tune, \n",
    "                embedding_size=embedding_size,\n",
    "                model_name = model_name,\n",
    "                pool=pool,\n",
    "                data_type=data_type)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    input = image_T.transform(image)\n",
    "    out = model(input.unsqueeze(dim=0).to(device))\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "pytorch model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "library = \"pytorch\"\n",
    "model_name = \"resnet34\"\n",
    "# model params\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "data_type=[\"rgb\"]\n",
    "\n",
    "image_T = Image_Transforms(model_name=model_name,\n",
    "                            library=library)\n",
    "\n",
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "input = image_T.transform(image)\n",
    "out = model(input.unsqueeze(dim=0).to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "annotations_file = \"/workdir/github/annotations_file_short_SF.csv\"\n",
    "dataset_dir = \"/workdir/sf_pv/data_v2\"\n",
    "train_type = 'train'\n",
    "data_type = ['wav']\n",
    "\n",
    "# model\n",
    "library = \"huggingface\"\n",
    "model_name = \"AST\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "\n",
    "audio_T = None\n",
    "image_T = None\n",
    "\n",
    "if 'wav' in data_type:\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "    audio_T = audio_T.transform\n",
    "\n",
    "if 'rgb' in data_type or 'thr' in data_type:\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    image_T = image_T.transform                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(annotations_file,\n",
    "                                     dataset_dir,'train',\n",
    "                                     data_type,\n",
    "                                     image_transform=image_T, \n",
    "                                     audio_transform=audio_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "HuggingFace model is used.\n",
      "AST model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "out = model(data.unsqueeze(dim=0).to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "annotations_file = \"/workdir/github/annotations_file_short_SF.csv\"\n",
    "dataset_dir = \"/workdir/sf_pv/data_v2\"\n",
    "train_type = 'train'\n",
    "data_type = ['rgb']\n",
    "\n",
    "# model\n",
    "library = \"timm\"\n",
    "model_name = \"resnet34\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "\n",
    "audio_T = None\n",
    "image_T = None\n",
    "\n",
    "if 'wav' in data_type:\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "    audio_T = audio_T.transform\n",
    "\n",
    "if 'rgb' in data_type or 'thr' in data_type:\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    image_T = image_T.transform                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(annotations_file,\n",
    "                                     dataset_dir,'train',\n",
    "                                     data_type,\n",
    "                                     image_transform=image_T, \n",
    "                                     audio_transform=audio_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "out = model(data.unsqueeze(dim=0).to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "annotations_file = \"/workdir/github/annotations_file_short_SF.csv\"\n",
    "path2datasets = \"/workdir/sf_pv\"\n",
    "dataset_dir = f\"{path2datasets}/data_v2\"\n",
    "train_type = 'train'\n",
    "data_type = ['rgb']\n",
    "\n",
    "# model\n",
    "library = \"timm\"\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "\n",
    "# transform\n",
    "audio_T = None\n",
    "image_T = None\n",
    "\n",
    "# sampler\n",
    "n_batch=10\n",
    "n_ways=2\n",
    "n_support=1\n",
    "n_query=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb data type\n",
      "timm model is used.\n"
     ]
    }
   ],
   "source": [
    "if 'wav' in data_type:\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "    audio_T = audio_T.transform\n",
    "\n",
    "if 'rgb' in data_type or 'thr' in data_type:\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    image_T = image_T.transform         \n",
    "\n",
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(annotations_file,dataset_dir,'train',\n",
    "                                image_transform=image_T, \n",
    "                                audio_transform=audio_T,\n",
    "                                data_type=data_type)\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(train_dataset.labels,\n",
    "                            n_batch,\n",
    "                            n_ways, # n_way\n",
    "                            n_support, # n_shots\n",
    "                            n_query)\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                        batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for batch in train_dataloader:\n",
    "    data_type = sorted(data_type)\n",
    "\n",
    "    if len(data_type) == 1:\n",
    "        data, label = batch\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        print(out.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = ValidDataset(path2datasets,'valid',\n",
    "                            image_transform=image_T, \n",
    "                            audio_transform=audio_T,\n",
    "                            data_type=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    data_type = sorted(data_type)\n",
    "\n",
    "    id1, id2, labels = batch\n",
    "\n",
    "    if len(data_type) == 1:\n",
    "        data_id1, _ = id1\n",
    "        data_id2, _ = id2\n",
    "\n",
    "        data_id1 = data_id1.to(device)\n",
    "        data_id2 = data_id2.to(device)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    id1_out = model(data_id1)\n",
    "    id2_out = model(data_id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "annotations_file = \"/workdir/github/annotations_file_short_SF.csv\"\n",
    "path2datasets = \"/workdir/sf_pv\"\n",
    "dataset_dir = f\"{path2datasets}/data_v2\"\n",
    "data_type = ['wav']\n",
    "\n",
    "# model\n",
    "library = \"huggingface\"\n",
    "model_name = \"AST\"\n",
    "pretrained_weights=True\n",
    "fine_tune=True\n",
    "embedding_size=128\n",
    "pool=None\n",
    "\n",
    "# transform\n",
    "audio_T = None\n",
    "image_T = None\n",
    "\n",
    "# sampler\n",
    "n_batch=10\n",
    "n_ways=2\n",
    "n_support=1\n",
    "n_query=1\n",
    "\n",
    "# loss\n",
    "dist_type='squared_euclidean'\n",
    "\n",
    "# train\n",
    "num_epochs=1\n",
    "save_dir='/workdir/results'\n",
    "exp_name='chern'\n",
    "wandb=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav data type\n",
      "HuggingFace model is used.\n",
      "AST model is used.\n"
     ]
    }
   ],
   "source": [
    "if 'wav' in data_type:\n",
    "    # audio transform params\n",
    "    sample_rate=16000\n",
    "    sample_duration=2 # seconds\n",
    "    n_fft=512 # from Korean code\n",
    "    win_length=400\n",
    "    hop_length=160\n",
    "    window_fn=torch.hamming_window\n",
    "    n_mels=40\n",
    "\n",
    "    audio_T = Audio_Transforms(sample_rate=sample_rate,\n",
    "                                sample_duration=sample_duration, # seconds\n",
    "                                n_fft=n_fft, # from Korean code\n",
    "                                win_length=win_length,\n",
    "                                hop_length=hop_length,\n",
    "                                window_fn=torch.hamming_window,\n",
    "                                n_mels=n_mels,\n",
    "                                model_name=model_name,\n",
    "                                library=library)\n",
    "    audio_T = audio_T.transform\n",
    "\n",
    "if 'rgb' in data_type or 'thr' in data_type:\n",
    "    image_T = Image_Transforms(model_name=model_name,\n",
    "                               library=library)\n",
    "\n",
    "    image_T = image_T.transform         \n",
    "\n",
    "model = Model(library=library, \n",
    "            pretrained_weights=pretrained_weights, \n",
    "            fine_tune=fine_tune, \n",
    "            embedding_size=embedding_size,\n",
    "            model_name = model_name,\n",
    "            pool=pool,\n",
    "            data_type=data_type)\n",
    "\n",
    "model = model.to(device)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = SpeakingFacesDataset(annotations_file,dataset_dir,'train',\n",
    "                                image_transform=image_T, \n",
    "                                audio_transform=audio_T,\n",
    "                                data_type=data_type)\n",
    "valid_dataset = ValidDataset(path2datasets,'valid',\n",
    "                            image_transform=image_T, \n",
    "                            audio_transform=audio_T,\n",
    "                            data_type=data_type)\n",
    "# sampler\n",
    "train_sampler = ProtoSampler(train_dataset.labels,\n",
    "                            n_batch,\n",
    "                            n_ways, # n_way\n",
    "                            n_support, # n_shots\n",
    "                            n_query)\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                        batch_sampler=train_sampler)\n",
    "\n",
    "\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "criterion = PrototypicalLoss(dist_type=dist_type)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train loss: 0.7720251381397247\n",
      "Average train accuracy: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(model=model,\n",
    "                    train_dataloader=train_dataloader, \n",
    "                    valid_dataloader=valid_dataloader,\n",
    "                    train_sampler=train_sampler,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    num_epochs=num_epochs,\n",
    "                    save_dir=save_dir,\n",
    "                    exp_name=exp_name,\n",
    "                    data_type=data_type,\n",
    "                    wandb=wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
